---
title: 每日三思（5月20日）
date: '2025-05-20 23:21:01'
updated: '2025-05-21 00:18:09'
permalink: /post/think-twice-every-day-may-20-pacri.html
comments: true
toc: true
---



# 每日三思（5月20日）

# 问题一：今天做了什么？

今天学习了朴素贝叶斯，对中级微观开始学习以及回顾，同时进行了翻译训练。

# 问题二：什么是朴素贝叶斯？

朴素贝叶斯是基于 **贝叶斯定理** 和 **特征条件独立性假设** 的概率分类算法。。它简单高效，广泛应用于文本分类、垃圾邮件识别、情感分析等场景。

**朴素贝叶斯算法（Naive Bayes）**  是一种基于 **贝叶斯定理** 和 **特征条件独立性假设** 的概率分类算法。它简单高效，广泛应用于文本分类、垃圾邮件识别、情感分析等场景。

## 一、基本原理

### 1. 贝叶斯定理回顾：

$$
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
$$

- $P(Y|X)$：在给定输入特征 $X$ 的情况下，属于类别 $Y$ 的后验概率。
- $P(Y)$：类别 $Y$ 的先验概率。
- $P(X|Y)$：似然，即在类别为 $Y$ 的条件下，出现特征 $X$ 的概率。
- $P(X)$：所有类别的特征分布的总和（可视为常数）。

> 任务是：对每一个类别计算 $P(Y|X)$，选择最大值作为预测结果。

### 2. “朴素”从何而来？

“**朴素**”是指算法对特征之间做了**条件独立性假设**：

> 假设所有特征在给定类别标签下是相互独立的。

虽然这个假设在现实中往往不成立（比如“天气热”和“喝冰饮”通常是相关的），但实践证明该算法在很多任务中仍然表现良好，尤其是文本分类。

## 二、算法流程（以多项式为例）

### 步骤如下：

1. 统计每个类别 $Y$ 出现的频率，得到先验概率 $P(Y)$。
2. 对于每一类 $Y$，统计每个特征 $x_i$ 在该类下的出现频率，得到似然 $P(x_i|Y)$。
3. 对新的样本 $X = (x_1, x_2, ..., x_n)$，根据贝叶斯公式计算：

    $$
    P(Y|X) \propto P(Y) \prod_{i=1}^{n} P(x_i|Y)
    $$
4. 选择使 $P(Y|X)$ 最大的类别作为预测结果。

## 三、常见的三种朴素贝叶斯模型

|类型|全称|适用数据类型|特点|
| ---------------| ------------------| --------------------| ------------------------------------------|
|BernoulliNB|伯努利朴素贝叶斯|二值特征（0/1）|适合稀疏的布尔特征，如文本是否包含某个词|
|MultinomialNB|多项式朴素贝叶斯|离散特征（如词频）|常用于文本分类，如垃圾邮件检测|
|GaussianNB|高斯朴素贝叶斯|连续特征|假设特征服从正态分布|

## 四、优点与缺点

### 优点：

- 训练和预测速度快。
- 对小规模数据和高维数据（如文本）效果好。
- 对缺失数据和噪声具有一定鲁棒性。

### 缺点：

- 条件独立性假设太强，在特征相关性强的情况下效果可能不佳。
- 对输入数据的分布敏感（尤其是高斯模型）。

# 问题三：极大似然估计（**MLE**）、贝叶斯估计（**Bayesian Estimation**）和极大后验估计（**MAP**）

## 一、基本概念回顾

我们考虑一个统计模型，其概率分布由参数 $\theta$ 决定，观测数据为 $D = \{x_1, x_2, ..., x_n\}$。我们的目标是：**根据观测数据 D 来估计参数** **$\theta$**。

## 二、三种估计方法的对比

|方法|全称|英文缩写|是否引入先验|输出结果类型|核心思想|
| --------------| ---------------------------------| ----------| --------------| ----------------------| ----------------------------------|
|极大似然估计|Maximum Likelihood Estimation|MLE|否|点估计|找出使观测数据出现概率最大的参数|
|贝叶斯估计|Bayesian Estimation|-|是|概率分布（后验分布）|把参数当作随机变量，求其后验分布|
|极大后验估计|Maximum A Posteriori Estimation|MAP|是|点估计|在先验知识下找出最可能的参数值|

## 三、原理详解

### 1. **极大似然估计（MLE）**

#### 原理：

寻找使观测数据 $D$ 出现概率最大的参数 $\theta$，即最大化似然函数：

$$
\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} P(D \mid \theta)
$$

- 不考虑任何关于 $\theta$ 的先验信息。只依赖于数据本身。

####  举例：

如果样本服从正态分布 $N(\mu, \sigma^2)$，那么 MLE 就是通过最大化样本似然函数来估计均值 $\mu$ 和方差 $\sigma^2$。

####  缺点：

- 当数据量小或噪声大时容易过拟合。并且对初始值敏感。

### 2. **贝叶斯估计（Bayesian Estimation）**

#### 原理：

将参数 $\theta$ 视为一个随机变量，并结合先验知识 $P(\theta)$，利用贝叶斯公式计算后验分布：

$$
P(\theta \mid D) = \frac{P(D \mid \theta) P(\theta)}{P(D)}
$$

- $P(D \mid \theta)$：似然
- $P(\theta)$：先验
- $P(\theta \mid D)$：后验
- $P(D)$：证据（常数）

贝叶斯估计的结果是一个完整的**后验分布**，而不是单一的数值。

#### 缺点：

- 计算复杂，尤其是高维参数空间。需要选择合适的先验分布。

### 3. **极大后验估计（MAP）**

#### 原理：

在贝叶斯框架下，MAP 是一种折中方案。它仍然是点估计，但考虑了先验信息：

$$
\hat{\theta}_{\text{MAP}} = \arg\max_{\theta} P(\theta \mid D) = \arg\max_{\theta} P(D \mid \theta) P(\theta)
$$

即最大化**后验概率**，等价于最大化**似然 × 先验**。

#### 说明：

- 如果先验是均匀分布（即无先验信息），则 MAP ≈ MLE。
- 可以看作带正则化的 MLE。

#### 缺点：

- 先验的选择会影响估计结果。
- 仍然只是一个点估计，不能反映不确定性。

## 四、通俗理解类比

|方法|类比|
| ------------| --------------------------------------------------------------|
|MLE|“只相信眼见为实”——完全依赖数据|
|MAP|“既听数据说话，也参考经验”——数据 + 先验|
|贝叶斯估计|“不仅知道最好是什么，还知道有多不确定”——给出完整信念分布|

## 五、实际应用建议

- **数据充足且无需先验** → 用 **MLE**
- **数据较少但有先验知识** → 用 **MAP**
- **需要量化不确定性 / 做预测区间** → 用 **贝叶斯估计**

‍
