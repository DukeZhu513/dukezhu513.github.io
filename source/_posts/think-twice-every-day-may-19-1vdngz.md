---
title: 每日三思（5月19日）
date: '2025-05-20 10:15:32'
categories:
  - 每日总结
tags:
  - 机器学习  
updated: '2025-05-20 11:50:08'
permalink: /post/think-twice-every-day-may-19-1vdngz.html
comments: true
toc: true
---



# 每日三思（5月19日）

# 问题一：今天做了什么？

今天主要进行了翻译训练，并且将SVM的全部内容包括线性SVM以及非线性SVM的代码实现全部学习完成。

# 问题二：怎么理解核函数？

在分类问题中，如果数据是**线性可分的**，我们可以用一个超平面将不同类别的样本分开。但在实际应用中，很多数据是**非线性可分的。** 这时，我们想到一个办法：

> **把数据从低维空间映射到高维空间，在更高维度上变得线性可分。**

比如，二维数据 $(x_1, x_2)$ 可以映射成三维特征 $(x_1^2, x_2^2, \sqrt{2}x_1x_2)$，这样就可能找到一个平面进行分割。但问题是：**映射到高维空间后计算代价很高**，特别是当维数非常大时。此时，我们就需要用到核函数了。它**不需要显式地把数据映射到高维空间，而是直接计算它们在高维空间中的内积。** ​

# 问题三：核函数的数学定义是怎么样的？

给定两个样本 $\mathbf{x}, \mathbf{x'}$，核函数 $K(\mathbf{x}, \mathbf{x'})$ 的定义是：

$$
K(\mathbf{x}, \mathbf{x'}) = \phi(\mathbf{x}) \cdot \phi(\mathbf{x'})
$$

其中：

- $\phi(\mathbf{x})$ 是将原始输入空间映射到高维空间的函数；
- “$\cdot$” 表示内积（点积）；
- 核函数的结果就是这两个映射后的向量的内积。

所以，我们**不用真的去算** **$\phi(\mathbf{x})$**，只需要知道它们的内积即可 —— 这大大节省了计算成本！

|核函数名称|公式|特点|
| ---------------------------------------------| ------| --------------------------------------|
|线性核（Linear Kernel）|$K(\mathbf{x}, \mathbf{x'}) = \mathbf{x}^T\mathbf{x'}$|最简单，适用于线性可分数据|
|多项式核（Polynomial Kernel）|$K(\mathbf{x}, \mathbf{x'}) = (\mathbf{x}^T\mathbf{x'} + c)^d$|能处理较复杂的非线性关系，参数多|
|径向基函数核 / RBF核（Gaussian RBF Kernel）|$K(\mathbf{x}, \mathbf{x'}) = \exp(-\gamma \|\mathbf{x} - \mathbf{x'}\|^2)$|最常用的核函数，适合大多数非线性问题|
|Sigmoid核|$K(\mathbf{x}, \mathbf{x'}) = \tanh(\kappa \mathbf{x}^T\mathbf{x'} + c)$|类似神经网络激活函数，有时用于二分类|

## 在 SVM 中

我们的目标是最大化分类间隔，优化的目标函数中会涉及到样本之间的**内积**。

例如：

$$
\max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n} y_i y_j \alpha_i \alpha_j (\mathbf{x}_i \cdot \mathbf{x}_j)
$$

如果我们使用核函数，就可以把里面的 $\mathbf{x}_i \cdot \mathbf{x}_j$ 替换成 $K(\mathbf{x}_i, \mathbf{x}_j)$。

也就是说，**整个优化过程都不需要知道具体的映射函数** **$\phi$**​ **，只需要知道核函数值即可**。

## SVM中，如何选择核函数？

|数据特点|推荐核函数|
| ----------------------| -----------------------|
|维度低且线性可分|线性核|
|图像、文本等复杂结构|RBF核（默认首选）|
|有明确多项式关系|多项式核|
|模拟神经网络效果|Sigmoid核（较少使用）|

可以想象：在整理一个房间中乱七八糟的东西（原始数据），想把它们分成两类（比如书和其他）：如果东西太杂乱，你可能要把它们放到一个更大的房间（高维空间）里才能分清楚；但是搬东西很麻烦，核函数就像有个聪明助手，他能告诉你：“其实你不一定要搬到大房间，只要你知道它们在大房间里怎么摆放（内积），就能判断怎么分。

# 问题四：什么是内积？为什么内积可以展示数据集在高维的分布情况？

两个向量 $\mathbf{a}, \mathbf{b}$ 的**内积**（也叫点积）是：

$$
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i
$$

### 内积的几何意义：

- 表示两个向量之间的**相似度**；
- 如果两个向量方向一致，内积大；
- 如果垂直（正交），内积为0；
- 如果方向相反，内积为负值。

## 二、内积与分类的关系？

在 SVM 中，我们要找一个最优超平面：

$$
f(\mathbf{x}) = \mathbf{w} \cdot \phi(\mathbf{x}) + b
$$

其中：

- $\phi(\mathbf{x})$ 是将输入映射到高维空间后的特征；
- $\mathbf{w}$ 是超平面的方向；
- $b$ 是偏置项。

为了找到这个 $\mathbf{w}$，我们需要计算不同样本之间的内积：$\phi(\mathbf{x}_i) \cdot \phi(\mathbf{x}_j)$

所以，**内积决定了决策边界如何形成** —— 它直接影响了模型对样本位置和类别的判断，反映了两个点之间的角度和距离关系，这种关系决定了它们在高维空间中的分布情况。

‍
