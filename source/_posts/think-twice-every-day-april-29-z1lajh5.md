---
title: 每日三思（4月29日）
date: '2025-04-30 00:06:03'
updated: '2025-04-30 01:00:47'
permalink: /post/think-twice-every-day-april-29-z1lajh5.html
comments: true
toc: true
---



# 每日三思（4月29日）

今日对机器学习开始系统的学习。南瓜书注重于公式的推导，欠缺了很多主干知识，所以我暂时是结合南瓜书和《机器学习实战》课程一起学习。对于机器学习，我想用大纲的方式将内容串联，因此在三思里会注重自我总结，而非公式推导和Python实现。

## 问题一：简要阐述线性回归与逻辑回归的差别？

虽然两者名字都带“回归”，但是它们的关注点有很大的不同。

**线性回归**：用于回归问题，即因变量是连续的，用来进行预测。

**逻辑回归**：用于分类问题，特别是二分类问题（例如是/否、0/1）。它通过预测类别的概率来进行分类决策。

## 问题二：简单介绍一下一元线性回归、多元线性回归以及多项式回归模型的公式以及特点

### 1. **一元线性回归（Simple Linear Regression）**

一元线性回归用于拟合一个自变量 $x$ 和因变量 $y$ 之间的关系，假设二者之间存在线性关系。其基本公式为：

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

* $y$：因变量（目标值）
* $x$：自变量（特征值）
* $\beta_0$：截距（bias）项
* $\beta_1$：斜率（slope），表示每增加一个单位的 $x$， $y$ 的变化量
* $\epsilon$：误差项，表示模型的预测值和真实值之间的差异

### 2. **多元线性回归（Multiple Linear Regression）**

多元线性回归用于描述多个自变量 $x_1, x_2, \dots, x_n$ 与因变量 $y$ 之间的线性关系。其基本公式为：

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n + \epsilon
$$

* $y$：因变量（目标值）
* $x_1, x_2, \dots, x_n$：自变量（特征值）
* $\beta_0$：截距（bias）项
* $\beta_1, \beta_2, \dots, \beta_n$：自变量的回归系数，表示每个自变量对因变量的影响
* $\epsilon$：误差项，表示模型的预测值和真实值之间的差异

### 3. **多项式回归（Polynomial Regression）**

多项式回归是线性回归的一种扩展，通过使用多项式关系来拟合非线性的数据。在多项式回归中，自变量 $x$ 的高次幂被引入模型。形如指数函数，其基本公式为：

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \dots + \beta_p x^p + \epsilon
$$

* $y$：因变量（目标值）
* $x$：自变量（特征值）
* $\beta_0, \beta_1, \dots, \beta_p$：回归系数，表示每个项对因变量的影响
* $p$：多项式的阶数，表示自变量 $x$ 的最高次幂
* $\epsilon$：误差项，表示模型的预测值和真实值之间的差异

### 4. **总结**

一元线性回归可以看为平面坐标轴的一条直线。对于多元线性回归，每多一个自变量需要多加一个维度，所以在维度过高时，选择先进行降维处理，再进行训练。多项式回归因为有高阶自变量是非线性回归，通过将高阶的自变量如：$x^2$看成$x_1$，以此类推，最终变为多元线性回归进行求解。

## 问题三：对线性回归中，最小二乘法和最大似然估计同时出现，在特定条件下得到的相同结果的思考

首先要明确线性回归本质上就是最优化问题。在回归模型中，如果我们假设误差项是服从正态分布的（即 $\epsilon \sim \mathcal{N}(0, \sigma^2)$），则最大似然估计可以推导出与最小二乘法相同的目标函数，即最小化误差的平方和。

最小二乘法是一种类似于“减少误差”的方法。像是在不断修正自己的预测，以减少与实际观测值之间的差距。它强调“准确性”和“误差最小化”，是直觉化的，没有考虑误差分布的情况。最大似然估计则类似于“寻找最可能的解释”或“最大化概率”的方法。假设我们已经知道数据是通过某种已知分布生成的（如正态分布），MLE会基于这个假设，找到最能符合数据生成过程的参数值。

最小二乘法基于优化和经验，最大似然估计另一种基于模型和推理。事实上误差分布确实大概率是符合正态分布的，两者基于不同的角度进行计算，最终得到相同的结果，互相提供支撑。也恰恰证明了**优化问题可能不仅仅是数学上的简化，而是与背后的概率模型紧密相关的**。

‍
